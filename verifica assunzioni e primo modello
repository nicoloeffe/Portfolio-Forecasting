
library(quantmod)
library(ggplot2)
library(e1071)
library(lmtest)
library(forecast)
library(tseries)
library(fitdistrplus)
library(dplyr)
library(gridExtra)


rm(list=ls())


#import dei dati
start_date = "2010-01-01"
end_date = Sys.Date()


# Lista dei simboli da scaricare
titoli = c("SPY", "AGG", "VNQ", "GLD")  # Aggiungi i titoli che ti interessano


# Ciclo per scaricare i dati per ogni titolo
for(titolo in titoli) {
  getSymbols(titolo, src = "yahoo", from = start_date, to = end_date)
}




###########################
### analisi esplorativa ###
###########################


x11()
par(mfrow = c(2, 2))  

plot(AGG$AGG.Adjusted,
     main = "Andamento dell'AGG Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "steelblue")

plot(GLD$GLD.Adjusted,
     main = "Andamento del GLD Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "lightgreen")

plot(SPY$SPY.Adjusted,
     main = "Andamento del SPY Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "lightpink")

plot(VNQ$VNQ.Adjusted,
     main = "Andamento del VNQ Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "lightblue")


## non sembra assolutamente essere una serie stazionaria c'è un trend e in generale l'andamento sembra 'modellabile a tratti', potrebbe aver senso dividere i diversi periodi
## e creare diversi modelli per ciascun momento (periodi di fiducia, periodi di recessione, di fatto figli di diverse popolazioni (mistura)). anche la volatilità sembra essere variabile.



x11()
plot(VNQ$VNQ.Adjusted + SPY$SPY.Adjusted + GLD$GLD.Adjusted + AGG$AGG.Adjusted,
     main = "Andamento del valore del portfolio Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "purple")

## problema: così segue di fatto il titolo dal prezzo maggiore e si perde le altre variazioni --> creiamo un prtafoglio che pesi i titoli in modo tale da avere lo stesso valore di ogni titolo (in partenza almeno)

graphics.off()


## troviamo coefficienti che distribuiscano equamente il valore del portafoglio su ciascun titolo (assumiamo che il portafoglio valga 400)

coeff_AGG = 100/as.numeric(AGG$AGG.Adjusted[1])
coeff_SPY = 100/as.numeric(SPY$SPY.Adjusted[1])
coeff_GLD = 100/as.numeric(GLD$GLD.Adjusted[1])
coeff_VNQ = 100/as.numeric(VNQ$VNQ.Adjusted[1])

x11()
plot(coeff_VNQ*VNQ$VNQ.Adjusted + coeff_SPY*SPY$SPY.Adjusted + coeff_GLD*GLD$GLD.Adjusted + coeff_AGG*AGG$AGG.Adjusted,
     main = "Andamento del valore del portfolio Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "purple")

## in questo modo il portafoglio è inizialmente equamente diviso in valore tra ciascun titolo (segue poi l'andamento di SPY perché questo titolo cresce in valore più di tutti gli altri)


## plotto istogrammi dei valori assunti da ciascun titolo

x11()
par(mfrow = c(2, 2))  

hist(AGG$AGG.Adjusted, breaks = 100, col = "steelblue", 
     main = "Distribuzione dei valori giornalieri AGG ", xlab = "Valore titolo")

hist(SPY$SPY.Adjusted, breaks = 100, col = "lightpink", 
     main = "Distribuzione dei rendimenti giornalieri SPY", xlab = "Rendimento")

hist(GLD$GLD.Adjusted, breaks = 100, col = "lightgreen", 
     main = "Distribuzione dei rendimenti giornalieri GLD", xlab = "Rendimento")

hist(VNQ$VNQ.Adjusted, breaks = 100, col = "lightblue", 
     main = "Distribuzione dei rendimenti giornalieri VNQ", xlab = "Rendimento")

## le distribuzione sono chiaramente multimodali, questo potrebbe essere dovuto a diversi regimi di mercato, ad esempio:
# - fasi di crescita economica (periodi di maggiore ottimismo e crescita)
# - periodi di recessione (periodi di alta volatilità e stress nei mercati)
# - periodi di stabilità del valore del titolo
# La multimodalità indica che i dati potrebbero provenire da diverse popolazioni o regimi economici (come diverse fasi del mercato finanziario). 
# Potremmo considerare la possibilità di suddividere i dati in periodi di espansione e contrazione economica. 
# Si puo vedere anche dal grafico della serie storica: ci sono periodi di scarsa volatilità in cui il valore del titolo si assesta attorno a un numero preciso.



## stessa cosa per il valore aggregato del portafoglio:

x11()
hist(coeff_VNQ*VNQ$VNQ.Adjusted + coeff_SPY*SPY$SPY.Adjusted + coeff_GLD*GLD$GLD.Adjusted + coeff_AGG*AGG$AGG.Adjusted, breaks = 100, col = "purple", 
     main = "Distribuzione dei rendimenti giornalieri portfoglio", xlab = "Rendimento")



graphics.off()


####################################################
### verifica assunzioni per la serie storica AGG: ##
####################################################

## per comodità racchiudo nell'oggetto 'serie' i dati che considero (in questo modo il codice rimane uguale, senza bisogno di andare a 
## sostituire con il nome di un altro titolo in varie parti del codice, sarà sufficiente riassegnare la variabile 'serie')

serie = as.data.frame(AGG)

serie$Data = index(AGG)

# Creiamo una matrice con solo la data e il valore Adjusted
serie = serie[, c("Data", "AGG.Adjusted")]

head(serie)




## spesso conviene studiare gli incrementi e non direttamente il valore del titolo (vediamo sotto perché)
incrementi = rep(NA, nrow(serie)-1)

for(i in 2:nrow(serie)) {
  incrementi[i-1] = serie[i,2]-serie[i-1,2]
}

serie_incrementi = serie[-1,]

serie_incrementi$AGG.Adjusted = incrementi

head(serie_incrementi)


## facciamo un paio di plot:

x11()
par(mfrow = c(2, 2))  

plot(serie,
     type = "l",
     main = "Andamento dell'AGG Adjusted PRICE",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "steelblue")

plot(serie_incrementi,
     type = "l",
     main = "Andamento degli INCREMENTI dell'AGG Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "darkgrey")

hist(serie$AGG.Adjusted, breaks = 100, col = "steelblue", 
     main = "Distribuzione dei valori giornalieri AGG ", xlab = "Valore titolo")

hist(serie_incrementi$AGG.Adjusted, breaks = 100, col = "darkgrey", 
     main = "Distribuzione dei rendimenti giornalieri AGG", xlab = "Rendimento")

## già si vede ad occhio che gli incrementi rispettano meglio le ipotesi di stazionarietà debole (e di distribuzione normale)
## di fatto sembra che la varianza non sia veramente costante nel tempo ma in ogni caso per i rendimenti l'ipotesi di stazionarietà
## è molto meno forte che per i valori dei titoli.

graphics.off()


###########################
## STAZIONARIETA DEBOLE? ##
###########################

## valutiamo quantitativamente le considerazioni qualitative appena fatte:

adf.test(serie[,2])

# dati NON stazionari (interpretazione e motivazione)

# guardiamo come sono fatti trend e stagionalità (ad occhio la presenza di un trend sembra innegabile):

serie_ts = ts(serie$AGG.Adjusted, frequency = 365)
# converto i dati in una serie temporale (TS)

decomposizione = decompose(serie_ts)

x11()
plot(decomposizione)  # mi sembra fin troppo maracta un stagionalità così, PUZZA!!


# la serie ha sia un trend evidente che una stagionalità, il che potrebbe indicare che un modello che cattura questi due effetti (come ARIMA con stagionalità, SARIMA, o un modello che combina trend e stagionalità) potrebbe funzionare bene.

# un modo per de-trendizzare la serie è sicuramente quello di passare ai rendimenti


# se passo ai rendimenti ho stazionarietà debole:

adf.test(serie_incrementi[,2])

##############################################################
## OSSERVAZIONI DISTRIBUITE MARGINALMENTE COME UNA NORMALE?###
##############################################################

# ?? siamo sicuri serva questa assunzione? nelle slide sta scritto così ma per quanto fatto fino ad ora non mi sembra che la normalità sia necessaria
# forse NON è NECESSARIA questa parte (peccato)

## analisi VALORI TITOLO

shapiro.test(serie[,2])  # H0: i dati sono distribuiti normalmente 
                         # pvalue basso rifiuto l'ipotesi nulla --> i dati NON sono distribuiti normalmente

x11()
qqnorm(serie[,2])  # creo Q-Q plot (confronto i quantili empirici dei dati con quelli teorici di una normale)
qqline(serie[,2]) #  linea di riferimento: i punti dovrebbero seguirla se fossero distribuiti normalmente

# sembrano essere approssimativamente normali nella parte centrale, ma deviano dalla normalità nelle code. 
# questo potrebbe significare che i dati hanno un comportamento più estremo in alcune situazioni (ad esempio, in periodi di volatilità elevata nei mercati finanziari)     

x11()
hist(serie[,2], breaks = 50, probability = TRUE, col = "lightblue", main = "Histogram with Normal Curve")
lines(density(serie[,2]), col = "red")  # Aggiungi la curva della densità
curve(dnorm(x, mean = mean(serie[,2]), sd = sd(serie[,2])), col = "darkgreen", add = TRUE)  # Curva normale teorica

## la distribuzione è chiaramente  multimodale, questo potrebbe essere dovuto a diversi regimi di mercato, ad esempio:
# - fasi di crescita economica (periodi di maggiore ottimismo e crescita)
# - periodi di recessione (periodi di alta volatilità e stress nei mercati)
# - periodi di stabilità del valore del titolo
# La multimodalità indica che i dati potrebbero provenire da diverse popolazioni o regimi economici (come diverse fasi del mercato finanziario). 
# Potremmo considerare la possibilità di suddividere i dati in periodi di espansione e contrazione economica, e testare la normalità separatamente per ciascun periodo.
# Si puo vedere anche dal grafico della serie storica: ci sono periodi di scarsa volatilità in cui il valore del titolo si assesta attorno a un numero preciso.

skewness(serie[,2]) # = -0.0027 --> la distribuzione dei dati è abbastanza simmetrica attorno alla sua media 
kurtosis(serie[,2]) # = -0.6973 < 0 --> la distribuzione ha una coda più leggera rispetto alla distribuzione normale.
                           # le code dei dati non sono particolarmente pesanti, ma potrebbero comunque esserci andamenti 
                           # anomali o periodi di volatilità elevata che non sono ben rappresentati da una distribuzione normale.


# dopo queste analisi concludiamo che la normalità dei dati è probabilmente un'assunzione forte




## analisi RENDIMENTI

shapiro.test(serie_incrementi[,2])  # H0: i dati sono distribuiti normalmente 
# pvalue basso rifiuto l'ipotesi nulla --> i dati NON sono distribuiti normalmente

x11()
qqnorm(serie_incrementi[,2])  # creo Q-Q plot (confronto i quantili empirici dei dati con quelli teorici di una normale)
qqline(serie_incrementi[,2]) #  linea di riferimento: i punti dovrebbero seguirla se fossero distribuiti normalmente

# sembrano essere approssimativamente normali nella parte centrale, ma deviano dalla normalità nelle code. 
# questo potrebbe significare che i dati hanno un comportamento più estremo in alcune situazioni (ad esempio, in periodi di volatilità elevata nei mercati finanziari)     

x11()
hist(serie_incrementi[,2], breaks = 50, probability = TRUE, col = "lightblue", main = "Histogram with Normal Curve")
lines(density(serie_incrementi[,2]), col = "red")  # Aggiungi la curva della densità
curve(dnorm(x, mean = mean(serie_incrementi[,2]), sd = sd(serie_incrementi[,2])), col = "darkgreen", add = TRUE)  # Curva normale teorica

## dai test precedenti risulta che l'assunzione di normalità sia ancora un'assunzione forte ma sembrano "più normali" dei valori
## giornalieri del titolo, i rendimenti (pur non essendo normali) ci si avvicinano di più.

skewness(serie_incrementi[,2]) # = -1.386 --> asimmetri negativa (distribuzione spostata verso sinistra), questo è tipico dei mercati finanziari: spesso i prezzi titolo subiscono cali più significativi rispetto ai rialzi.
kurtosis(serie_incrementi[,2]) # = 27.06 < 0 --> code molto più pesanti rispetto a una distribuzione normale. Ciò suggerisce la presenza di eventi rari (come i forti picchi o cali nei prezzi), che sono tipici nei mercati finanziari.


## l'assunzione di normalità sembra essere forte anche in questo caso, la distribuzione dei rendimenti non può essere approssimata con una normale
## perché la distribuzione dei dati segue una distribuzione che consente eventi estremi più frequentemente di quanto prevederebbe una normale.
## nel nostro studio non siamo però interessati a prevedere gli eventi estremi ma solamente a fare previsioni su intervalli di tempi vicini: vogliamo fare mediamente delle buone previsioni, non importa sbagliare di grosso occasionalmente (statistici non trader)

## in conclusione è meglio utilizzare i dati relativi ai rendimenti per i modelli, non quelli dei valori del titolo

##################################################
## autocorrelazione e autocorellazione parziale ##
##################################################


## plottiamo autocorrelazione e autocorrelazione parziale dei valori giornalieri del titolo:

x11()
par(mfrow = c(1, 2)) 

acf(serie[,2], main = "Correlogramma VALORE AGG", lag.max = 100)
# ogni osservazione dipende fortemente dalle precedenti, i dati sembrano seguire una forte relazione temporale,
# un modello di serie temporale potrebbe effettivamente essere utile per modellare i dati

pacf(serie[,2], main = "Correlogramma parziale VALORE AGG", lag.max = 100)
# c'è una forte autocorrelazione tra il valore corrente e il valore precedente ma dopo il lag 1, le correlazioni diventano molto più piccole:
# la mancanza di significatività a lag più alti suggerisce che i dati non dipendono da lag più lontani.



## plottiamo ora autocorrelazione e autocorrelazione parziale dei rendimenti giornalieri:

x11()
par(mfrow = c(1, 2)) 

acf(serie_incrementi[,2], main = "Correlogramma INCREMENTI AGG", lag.max = 100)
# la correlazione non è significativa per lag maggiori di 0: usare l'incremento di oggi per spiegare quello di domani potrebbe NON essere una buona idea

pacf(serie_incrementi[,2], main = "Correlogramma parziale INCREMENTI AGG", lag.max = 100)



## penso che la forte correlazione temporale tra i valori giornalieri del titolo sia solo apparenza (poca sostanza). 
## mi spiego meglio: forse se i rendimenti non sono correlati significa che fare previsione per domani è difficile, 
## la correlazione tra il valore del titolo oggi e domani dice solo che se oggi vale 100 domani varrà ancora più o meno così 
## (quindi l'unica cosa che mi dice è che mediamente il rendimento giornaliero è piccolo, non sa però stimare il rendimento di domani guardando i rendimenti passati) 


# riassumendo quanto visto fino ad ora:
# - i dati relativi al valore giornaliero del titolo non rispettano l'assunzione di stazionarietà mentre quelli dei rendimenti (quasi) si 
# - i valori giornalieri sono temporalmente correlati tra loro (tutta apparenza), i rendimenti non presentano correlazioni temporali significative (purtroppo) (soldi facili per tutti altrimenti)
#                                                             (mi spiego meglio nelle righe 436 e seguenti)

#####################################################################
## DEFINIZIONE FINESTRE PER TRAIN E TEST DEL MODELLO - possibilità ##
#####################################################################

## diversi modi per dividere i dati in train e test set, mi interessa definire qua delle funzioni che possano essere
## usate per dividere il dataset in varie finestre (ciascuna con dati di train e dati di test)

# possibili idee per individuare le finestre (avendo scelto su quanti giorni allenare il modello e su quanti fare previsione):
# - finestre attigue (disgiunte) dalla lunghezza costante
# - finestre scorrevoli (da decidere di quanto traslare la finestra ogni volta) (non più disgiunte ma parzialmente sovrapposte)
# - N finestre prese in modo randomico sull'orizzonte temporale (alcune totalmente disgiunte, altre parzialmente sovrapposte, tutto casuale)



# funzione per creare finestre disgiunte:

non_overlapping_windows = function(dataset_completo, train_size, test_size) 
{
  windows = list()
  
  for (start in seq(1, nrow(dataset_completo) - (train_size + test_size), by = train_size + test_size)) 
  {
    end_train = start + train_size - 1
    start_test = end_train + 1
    end_test = start_test + test_size - 1
    
    windows[[length(windows) + 1]] = list(
      train = dataset_completo[start:end_train,2],
      test = dataset_completo[start_test:end_test,2]
    )
  }
  
  return(windows)
}



# funzione per creare finestre scorrevoli:

sliding_windows = function(dataset_completo, train_size, test_size, step_size) 
  {
    windows = list()
  
    for (start in seq(1, nrow(dataset_completo) - (train_size + test_size), by = step_size)) 
      {
        end_train = start + train_size - 1
        start_test = end_train + 1
        end_test = start_test + test_size - 1
        
        windows[[length(windows) + 1]] = list(
          train = dataset_completo[start:end_train,2],
          test = dataset_completo[start_test:end_test,2]
        )
      }
    
  return(windows)
}



# funzione per creare finestre campionate casualmente:

random_windows = function(dataset_completo, train_size, test_size, number_of_windows) 
{
  windows = list()
  
  for (i in 1:number_of_windows) 
  {
    start_train = sample(1:(nrow(dataset_completo) - (train_size + test_size)), 1)
    end_train = start_train + train_size - 1
    start_test = end_train + 1
    end_test = start_test + test_size - 1
    
    windows[[length(windows) + 1]] = list(
      train = dataset_completo[start_train:end_train,2],
      test = dataset_completo[start_test:end_test,2]
    )
  }
  
  return(windows)
}



##########################
## PRIMO MODELLO: AR(1) ##
##########################

## N.B: abbiamo visto prima che la serie dei valori giornalieri non è stazionaria, sarebbe una scelta più felice quella di usare i rendimenti (esattamente ciò che faremo poi)
## voglio però prima mostrare come la forte correlazione tra i valori giornalieri del titolo di fatto non sia così utile (dice solo che i rendimenti sono piccoli in media)

## qua di fatto giustifico l'insinuazione fatta a riga 343 (correlazione forte ma informazione solo apparentemente significativa)

# creiamo una regressione AR(1) e analizziamo il modello risultante:

valore_precedente = serie[1:nrow(serie)-1,2] # escludo l'ultimo valore (non è il valore precedente di nessun altro)

primo_modello = lm(serie[-1,2] ~ valore_precedente)
# escludo il primo valore (non ha valori precedenti che possiamo usare per stimarlo)

# visualizziamo i parametri stimati:
summary(primo_modello)
# Rquadro elevato, il modello sembrerebbe spiegare bene la variabilità dei dati
# coeff significativo e MOLTO vicino a 1 --> in linea con quanto ci saremmo aspettati (per questo modello poco utile, sostanzialmente per domani predice il valore di oggi)
# intercetta significativa ma bassa, sembra indicare una leggera tendenza positiva ( in assenza di altri fattori, il valore del titolo ha una piccola crescita costante nel tempo)

## il valore precedente è molto significativo, l'elevata correlazione tra valori vicini è quindi confermata dal modello.
## quando dico che questa è tutta apparenza intendo dire che di fatto l'evidenza raccolta da questo modello non è nulla di sorprendente: a meno di eventi straordinari il valore di domani assomiglierà a quello di oggi (incrementi giornalieri piccoli in media)


# confrontiamo valori fittati e valori reali
fitted_values = c((0), primo_modello$fitted.values)

x11()
plot(1:10, serie[1:10,2],
     pch   = 16,          # cerchi pieni
     ylab  = "Valore",
     main  = "Valore reale vs. Fitted",
     xaxt  = "n")         # disattiva asse x automatico
axis(1, at = 1:10, labels = 1:10)
abline(v = 1:10, col = "lightgray", lty = 3)
points(1:10, fitted_values[1:10],
       pch  = 17,         # triangoli pieni
       col  = 3)
legend("topleft",
       legend = c("Osservato", "Fittato"),
       col    = c("1", "3"),
       pch    = c(16, 17),
       lty    = 1,
       bty    = "n")

## il modello è estremamente scadente: predice per domani il valore di oggi 
## viene il dubbio che includere altre isservazioni oltre a quella del giorno immediatamente 
## precedente potrebbe migliorare le cose (no! infatti, la PACF risulti “piatta” dopo il primo picco 
## proprio perché una volta usato il valore immediatamente precedente per spiegare quello successivo
## tutti i valori che vengono ancora prima non sono più significativamente correlati con quello considerato)

## mostriamo come a prima vista un modello lineare possa non sembrare una pessima idea pur presentando i suoi limiti:

coordinate = cbind(valore_precedente, serie[-1, 2])

x11()
plot(coordinate)
abline(a = primo_modello$coefficients[1], b = primo_modello$coefficients[2], lwd = 3, col = 3)


graphics.off()


# L'analisi del modello sarà così strutturata:
# - diagnostica dei residui
# - costruzione intervalli di previsione
# - impatto della dimensione del train test
# - valutazione della bontà del modello sui test set

# mostreremo infine quanto di fatto tale modello non riesca a catturare alcuna informazione particolarmente utile
# confrontandolo con un modello estremamente naif (valore_domani = valore_oggi + rnorm(mu_incrementi, sd_incrementi)) 

#########################################
## PRIMO MODELLO - diagnostica residui ##
#########################################

summary(primo_modello)
# a occhio i residui non sembrano problematici ma facciamo una diagnostica più precisa:

# Calcoliamo i residui e grafichiamoli
residui = primo_modello$residuals

x11()
plot(residui, 
     main = "Residui", 
     ylab = "Residui")
abline(h = 0, col = 2, lwd = 2)
# sembrerebbero grosso modo simmetrici intorno allo 0
# i valori estremi di errore sono associati a un crollo improvviso del valore del titolo, non si poteva prevedere
# non propriamente stazionari (a occhio, poi approfondiamo), ce lo aspettavamo (la volatilità del mercato è variabile)


## guardiamo meglio cosa succede in corrispondenza di quei valori di residui elevati:

x11()
plot((which.max(residui)-10):(which.max(residui)+10), serie[(which.max(residui)-10):(which.max(residui)+10),2],
     pch   = 16,
     type = 'b',
     ylab  = "Valore",
     main  = "Valore reale vs. Fitted")
abline(v = (which.max(residui)-10):(which.max(residui)+10), col = "lightgray", lty = 3)
points((which.max(residui)-10):(which.max(residui)+10), fitted_values[(which.max(residui)-11):(which.max(residui)+9)],
       pch  = 17,         # triangoli pieni
       col  = 3)
legend("topright",
       legend = c("Osservato", "Fittato"),
       col    = c("1", "3"),
       pch    = c(16, 17),
       lty    = 1,
       bty    = "n")

# come ci aspettavamo questa serie di residui elevati dipende da un momento di elevata volatilità del valore del titolo,
# un modello semplice come l'AR(1) considerato non è in grado di considerare questo fattore nella sua previsione

graphics.off()



## RESIDUI NORMALI?

# Grafico Q-Q per verificare la normalità dei residui:
x11()
qqnorm(residui)
qqline(residui, col = "red")
# deviazioni dalle code normali suggeriscono la presenza di eventi estremi che il modello non può catturare completamente (giornate matte del mercato)

# non è un problema se il modello non è perfetto nella previsione di eventi estremi, l'importante è che offra un buon adattamento complessivo, 
# considerando che nel contesto finanziario gli eventi estremi sono intrinsecamente difficili da prevedere. L'importante è che il modello 
# riesca a catturare la tendenza generale e a gestire le fluttuazioni normali del mercato


shapiro.test(residui) # H0: residui normali, p-value basso --> rifiuto H0 (normalità dei residui ipotesi forte)

x11()
hist(residui, breaks = 50, probability = TRUE, col = "lightblue", main = "Histogram with Normal Curve")
lines(density(residui), col = "red")  # Aggiungi la curva della densità
curve(dnorm(x, mean = mean(residui), sd = sd(residui)), col = "darkgreen", add = TRUE)  # Curva normale teorica
legend("topright", 
       legend = c("Densità empirica dei residui", "Densità Normale Teorica"), 
       col = c("red", "darkgreen"), 
       lty = 1, 
       cex = 0.8)

## più stretta al centro ma code pesanti, la normale non sembra approssimare been la distribuzione dei residui
## lo vediamo anche da skewness e kurtosis:


skewness(residui) # = -1.3391 --> asimmetria negativa (distribuzione spostata verso sinistra), tipico dei mercati finanziari: spesso i prezzi subiscono cali più significativi rispetto ai rialzi
kurtosis(residui) # = 26.691 >> 0 --> la distribuzione ha code molto più pesanti rispetto alla distribuzione normale.

## confermo che assumere la normalità dei residui potrebbe essere un'ipotesi forte, ribadisco però quanto 
## scritto in precedenza: le giornate matte dei mercati (corrispondenti a residui particolarmente alti e responsabili dello spessore delle code) 
## non sono ciò che il nostro modello vuole spiegare. alla luce di questa considerazione potrebbe essere obiettato che quelle osservazione
## dovrebbero essere tolte dal dataset ma a questa obiezione risponderei così:
## "non stiamo costruendo questo modello perché pensiamo possa spiegare bene i dati, è un modello estremamente semplice, ci interessa usarlo solo per far emergere degli aspetti dell'analisi che troviamo interessanti. 
##  l'obiettivo a questo punto del lavoro è solo quello di fare osservazioni opportune e rilevanti, solo più avanti proveremo a usare modelli più sofisticati per spiegare meglio i dati".


# Calcoliamo e plottiamo l'autocorrelazione dei residui
x11()
acf(residui, main = "Autocorrelazione dei Residui") 
# residui scorrelati, bene

# Test di Breusch-Pagan per eteroscedasticità:
bptest(primo_modello)  # P-value bassissimo --> rifiuto H0, residui eteroschedastici
                       # è quello che dicevamo prima, la volatilità del mercato è variabile, questo
                       # si ripercuote sul modulo degli errori.

# vediamo se, come suggeriva il plot fatto prima, i residui potrebbero essere considerati omoschedastici
# fino a poco prima del momento di massimo errore del modello:
bptest(serie[2:(which.max(residui)-20),2] ~ valore_precedente[1:(which.max(residui)-21)])

# p-value = 0.1825  --> possiamo considerare i residui omoschedastici fino a prima di quel momento


## IN CONCLUSIONE:
## residui indipendenti ma NON normali (e nemmeno identicamente distribuiti, eteroschedasticità (omoschedasticità spolo locale))

## ??possiamo comunque usare il modello lineare??

###########################################################
## PRIMO MODELLO - funzione valutazione metrica di bontà ##
###########################################################


## la principale metrica che utilizzeremo per confrontare la bontà dei modelli costruiti 
## sarà la capacità del modello di produrre intervalli di previsione che contengono i valori veramente osservati


## DUE vie per individuare degli intervalli di previsione per i valori successivi:

# 1) approccio frequentista: l'intervallo di previsione (simmetrico) viene centrato sulla previsione puntuale fatta dal modello 
#    e la sua ampiezza viene determinata a partire dalla varianza dell'errore (assunto normale) e dal quantile legato al livello di confidenza richiesto per la previsione 

# 2) approccio bayesiano: consideriamo le osservazioni future come v.a. e utilizziamo la distribuzione a posteriori per calcolare gli intervalli di previsione. 

# ???? chiedere a mastrantonio se abbia senso scomodare l'approccio bayesiano ????



## scrivo quindi qua sotto una funzione che possa essere utilizzata più avanti per valutare i diversi modelli rispetto a questa metrica
## per ora ho scritto solo la funzione che crea gli intervalli come farebbe un frequentista

evaluate_intervals <- function(modello, alpha) {
  
  # modello: oggetto lm AR(1)
  # alpha: livello di significatività (es. 0.05)

  # Previsione puntuale e residui
  previsione_puntuale = modello$fitted.values
  residui = modello$residuals
  
  # Calcolo una stima della deviazione standard e della media dei residui:
  sd_residui = sd(residui)
  mean_residui = mean(residui) 
  
  # Determinazione del valore critico per un intervallo di confidenza di livello 1-alpha (siamo sotto l'assunzione (forte) valutata prima di normalità dei residui)
  quantile = qnorm(1 - alpha/2, mean_residui, sd_residui) 
  
  
  n = nobs(modello)
  
  # Pre-allocazione
  limite_superiore =  rep(NA, n)
  limite_inferiore = rep(NA, n)
  storia_crescita_valori_fuori_int = rep(NA, n)
  
  # Calcoliamo l'intervallo di previsione 
  limite_superiore = previsione_puntuale + quantile*sd_residui
  limite_inferiore = previsione_puntuale - quantile*sd_residui
  
  
  # Calcolo interi a partire dal secondo punto
  valori_fuori_intervallo = 0
  storia_crescita_valori_fuori_int[1] = 0
  
  for(i in 1:nobs(modello)) {
    
    # verifico se l'osservazione corrente è fuori intervallo
    if(model.frame(modello)[i,1] < limite_inferiore[i] | model.frame(modello)[i,1] > limite_superiore[i]) {
      valori_fuori_intervallo = valori_fuori_intervallo + 1
    }
    
    storia_crescita_valori_fuori_int[i] = valori_fuori_intervallo
  }
  
  percentuale_valori_fuori_int = valori_fuori_intervallo / n
  
  return(list(
    limite_superiore = limite_superiore,
    limite_inferiore = limite_inferiore,
    percentuale_valori_fuori_int = percentuale_valori_fuori_int,
    storia_crescita_valori_fuori_int = storia_crescita_valori_fuori_int
  ))
}


## vediamo se la funzione fa quello che ci aspetteremmo

risultati_prova = evaluate_intervals(primo_modello, 0.05)
alpha = 0.05

# Visualizziamo il risultato degli intervalli restituiti vs i valori osservati
x11()
plot(serie[2:50,2], col = 1, pch = 16, cex = 0.5)
lines(risultati_prova$limite_inferiore[1:50], col = 2, type = 'l')
lines(risultati_prova$limite_superiore[1:50], col = 2, type = 'l')
legend("topleft",
       legend = c("Serie reale", 1 - alpha),
       col    = c(1, 2),
       pch    = c(16, NA),
       lty    = c(NA, 1),
       pt.cex = c(0.5, NA),
       bty    = "n")


graphics.off()




## ??? DOVREI/POTREI CONSIDERARE ANCHE L'INCERTEZZA NELLA STIMA DEI PARAMETRI? (GPT DICEVA DI AGGIUNGERE UN TERMINE LEGATO ALL'ERRORE STANDARD NELLA STIMA DEI PARAMETRI DEL MODELLO)




## confrontiamo la percentuale osservata di valori fuori dall'int con la percentuale teorica (alpha):
cat(sprintf("Alpha teorico: %.2f, Percentuale osservata di valori fuori intervallo: %.2f",
            alpha, risultati_prova$percentuale_valori_fuori_int))

## percentuale osservata >>> percentuale teorica --> gli intervalli, seppur siano molto larghi, non lo sono abbastanza!
## tale discrepanza è dovuta al fatto che i residui non si distribuiscono davvero normalmente: le code della loro distribuzione
## sono molto più pesanti (ricordo i plot e il valore elevatissimo di kurtosis)

## una percentuale così alta di valori fuori intervallo mi sorprende, è ancora più alta di quanto mi sarei aspettato 
## proviamo a vedere come si distribuiscono le osservazioni fuori intervallo:

idx = seq_along(risultati_prova$storia_crescita_valori_fuori_int)

x11()
plot(idx, risultati_prova$storia_crescita_valori_fuori_int,
     type  = "l",
     lwd   = 2,
     col   = 1,
     ylab  = "Conteggio cumulato osservazioni fuori IP",
     panel.first = grid(col = "lightgray", lty = "dotted"))
abline(a = 0, b = 0.33,
       col  = "forestgreen",
       lwd  = 2,
       lty  = "dashed")
v0 = which.max(primo_modello$residuals)
abline(v = v0,
       col  = "steelblue",
       lwd  = 2,
       lty  = "dotdash")
legend("topleft",
       legend = c("# osservaizoni fuori-IP", "Punto individuato da diagnostica residui"),
       col    = c(1, "steelblue"),
       lty    = c("solid", "dotdash"),
       lwd    = c(2, 2, 2),
       bty    = "n")

# La linea verticale blu segna il punto in cui la variabilità dei residui cresce (v. grafico diagnostica) 
# Prima di quel punto, i residui possono essere considerati omoschedastici e il ritmo di crescita dei fuori‑intervallo cresce quasi in linea con l’aspettativa;
# dopo, invece, la pendenza aumenta nettamente, riflettendo il passaggio a una fase di volatilità più elevata

# confrontiamo la frequenza di fuori intervallo osservata nella prima porzione di dati con quella teorica (alpha):

valori_fuori_intervallo = 0
storia_crescita_valori_fuori_int = rep(NA, (which.max(residui)-20))
semi_ampiezza_int = 0.5*(risultati_prova$limite_superiore[1] - risultati_prova$limite_inferiore[1])

for(i in 2:(which.max(residui)-20))
{
  if(serie[i,2] > primo_modello$fitted.values[i-1] + semi_ampiezza_int | serie[i,2] < primo_modello$fitted.values[i-1] - semi_ampiezza_int)
    valori_fuori_intervallo = valori_fuori_intervallo + 1
  
  storia_crescita_valori_fuori_int[i] = valori_fuori_intervallo
}

percentuale_valori_fuori_int = valori_fuori_intervallo/(which.max(residui)-20)

cat(sprintf("Alpha teorico: %.2f, Percentuale osservata di valori fuori intervallo: %.2f",
            alpha, percentuale_valori_fuori_int))

## meglio ma comunque molto maggiore della teorica, penso sia dovuto al fatto che i residui non sono davvero normali (code pesanti)



## ???? forse ha senso andare a rivedere cosa avevamo fatto nell'esercitazione dei dugonghi... avevamo creato delle bande di previsione per un modello lineare anche lì 


######################################
## PRIMO MODELLO - dimensione train ##
######################################


## Il modello AR(1) costruito sopra usa tutte le osservazioni disponibili per stimare i parametri ma questo potrebbe peggiorare le sue performance:
## se la dinamica del titolo cambia (ad es. inizia una nuova fase di volatilità), i dati troppo vecchi possono essere fuorvianti

# siamo quindi interessati a vedere se costruire il modello su 100, 1000 o 3000 dati faccia differenza (e quale dimensione del train risulti migliore per descrivere i dati)

## N.B. non è detto che il modello migliore per descrivere sia anche quello migliore per predirre.
##      confronteremo i risultati trovati qui con quelli che otterremo nella parte di valutazione del modello predittivo


# definisco il numero di finestre che campionerò (ovvero il numero di modelli che fitterò)
num_windows = 1000



## dimensione train = 100:

finestre_100 = random_windows(serie, 100, 0, num_windows)
# campiono diverse finestre di train

## ???? e se le finestre non le prendessi in modo randomico? 

# fitto un modello per ciascuna finestra (di ciascun modello salvo summary, residui e fitted values):

results_100 = vector("list", num_windows)

for(i in 1:num_windows) {
  
  train = finestre_100[[i]]$train   
  
  valori = train[-1]          
  predittori = train[-length(train)]
  
  mod = lm(valori ~ predittori)
  
  risultati_intervalli = evaluate_intervals(mod, alpha)
  
  # Salvo summary, residui e fitted.values
  results_100[[i]] = list(
    summary       = summary(mod),
    residuals     = residuals(mod),
    fitted.values = fitted.values(mod),
    percentuale_valori_fuori_int = risultati_intervalli$percentuale_valori_fuori_int
  )
}

# esempio di accesso ai dati salvati nella lista:
results_100[[10]]$summary
results_100[[7]]$residuals
results_100[[1]]$fitted.values
results_100[[41]]$percentuale_valori_fuori_int




## dimensione train = 500:

finestre_500 = random_windows(serie, 500, 0, num_windows)
# campiono diverse finestre di train

# fitto un modello per ciascuna finestra (di ciascun modello salvo summary, residui e fitted values):

results_500 = vector("list", num_windows)

for(i in 1:num_windows) {
  
  train = finestre_500[[i]]$train   
  
  valori = train[-1]          
  predittori = train[-length(train)]
  
  mod = lm(valori ~ predittori)
  
  risultati_intervalli = evaluate_intervals(mod, alpha)
  
  # Salvo summary, residui e fitted.values
  results_500[[i]] = list(
    summary       = summary(mod),
    residuals     = residuals(mod),
    fitted.values = fitted.values(mod),
    percentuale_valori_fuori_int = risultati_intervalli$percentuale_valori_fuori_int
  )
}




## dimensione train = 1000:

finestre_1000 = random_windows(serie, 1000, 0, num_windows)
# campiono diverse finestre di train

# fitto un modello per ciascuna finestra (di ciascun modello salvo summary, residui e fitted values):

results_1000 = vector("list", num_windows)

for(i in 1:num_windows) {
  
  train = finestre_1000[[i]]$train   
  
  valori = train[-1]          
  predittori = train[-length(train)]
  
  mod = lm(valori ~ predittori)
  
  risultati_intervalli = evaluate_intervals(mod, alpha)
  
  # Salvo summary, residui e fitted.values
  results_1000[[i]] = list(
    summary       = summary(mod),
    residuals     = residuals(mod),
    fitted.values = fitted.values(mod),
    percentuale_valori_fuori_int = risultati_intervalli$percentuale_valori_fuori_int
    
  )
}



## dimensione train = 3000:

finestre_3000 = random_windows(serie, 3000, 0, num_windows)
# campiono diverse finestre di train

# fitto un modello per ciascuna finestra (di ciascun modello salvo summary, residui e fitted values):

results_3000 = vector("list", num_windows)

for(i in 1:num_windows) {
  
  train = finestre_3000[[i]]$train   
  
  valori = train[-1]          
  predittori = train[-length(train)]
  
  mod = lm(valori ~ predittori)
  
  risultati_intervalli = evaluate_intervals(mod, alpha)
  
  # Salvo summary, residui e fitted.values
  results_3000[[i]] = list(
    summary       = summary(mod),
    residuals     = residuals(mod),
    fitted.values = fitted.values(mod),
    percentuale_valori_fuori_int = risultati_intervalli$percentuale_valori_fuori_int

  )
}


# confronto i valori di beta1, R^2 ed errore std restituiti dai modelli di diverse dimensioni:

metrics_100 <- lapply(results_100, function(res) {
  s <- res$summary
  data.frame(
    beta1                   = s$coefficients["predittori", "Estimate"],
    R2                      = s$adj.r.squared,
    residual_se             = s$sigma
  )
}) %>% bind_rows()

metrics_500 = lapply(results_500, function(res) {
  s     = res$summary
  data.frame(beta1 = s$coefficients["predittori", "Estimate"],
             R2    = s$adj.r.squared,
             residual_se = s$sigma)
}) %>% bind_rows()

metrics_1000 = lapply(results_1000, function(res) {
  s     = res$summary
  data.frame(beta1 = s$coefficients["predittori", "Estimate"],
             R2    = s$adj.r.squared,
             residual_se = s$sigma)
}) %>% bind_rows()

metrics_3000 = lapply(results_3000, function(res) {
  s     = res$summary
  data.frame(beta1 = s$coefficients["predittori", "Estimate"],
             R2    = s$adj.r.squared,
             residual_se = s$sigma)
}) %>% bind_rows()



# definisco una funzione che userò sotto per fare tutti i grafici del caso (senza era un caos):
make_hist <- function(vec, colore, titolo) {
  mean_val <- mean(vec)
  ggplot(data.frame(x = vec), aes(x)) +
    geom_histogram(bins = 15, fill = colore, color = "white") +
    geom_vline(xintercept = mean_val, linetype = "dashed", size = 1, color = colore) +
    annotate("text", x = mean_val, y = Inf, label = sprintf("%.3f", mean_val),
             vjust =  2, color = colore, size = 3) +
    labs(title = titolo) +
    theme_minimal() +
    theme(
      axis.title   = element_blank(),
      axis.text.y  = element_blank(),
      axis.ticks.y = element_blank(),
      legend.position = "none"
    )
}





# valori del residual standard error:

residual_se_hist_100  = make_hist(metrics_100$residual_se,  "pink", "Residual Standard Error (100)")
residual_se_hist_500  = make_hist(metrics_500$residual_se,  "lightgreen", "Residual Standard Error (500)")
residual_se_hist_1000  = make_hist(metrics_1000$residual_se,  "steelblue", "Residual Standard Error (1000)")
residual_se_hist_3000  = make_hist(metrics_3000$residual_se,  "salmon", "Residual Standard Error (3000)")


x11()
grid.arrange(
  residual_se_hist_100, 
  residual_se_hist_500, 
  residual_se_hist_1000, 
  residual_se_hist_3000,
  ncol = 2, nrow = 2
)


# valori di beta1:

beta1_values_hist_100  = make_hist(metrics_100$beta1,  "pink", "Distribuzione di β₁ (100)")
beta1_values_hist_500  = make_hist(metrics_500$beta1,  "lightgreen", "Distribuzione di β₁ (500)")
beta1_values_hist_1000  = make_hist(metrics_1000$beta1,  "steelblue", "Distribuzione di β₁ (1000)")
beta1_values_hist_3000  = make_hist(metrics_3000$beta1,  "salmon", "Distribuzione di β₁ (3000)")


x11()
grid.arrange(
  beta1_values_hist_100,
  beta1_values_hist_500,
  beta1_values_hist_1000,
  beta1_values_hist_3000,
  ncol = 2, nrow = 2
)




# valori R^2 aggiustato:

R2_values_hist_100  = make_hist(metrics_100$R2,  "pink", "Distribuzione di Adjusted R²₁ (100)")
R2_values_hist_500  = make_hist(metrics_500$R2,  "lightgreen", "Distribuzione di Adjusted R²₁ (500)")
R2_values_hist_1000  = make_hist(metrics_1000$R2,  "steelblue", "Distribuzione di Adjusted R²₁ (1000)")
R2_values_hist_3000  = make_hist(metrics_3000$R2,  "salmon", "Distribuzione di Adjusted R²₁ (3000)")

x11()
grid.arrange(
  R2_values_hist_100,
  R2_values_hist_500,
  R2_values_hist_1000,
  R2_values_hist_3000,
  ncol = 2, nrow = 2
)


# in sostanza vediamo come il valore medio di ciascun parametro differisca di poco cambiando la dimensione
# del train, la varianza come ci si può aspettare diminuisce al cresceredella dimensione del train
# unico parametro che migliora significativamente al crescere della dimensione è R^2

## importante notare come Residual Standard Error (ovvero la deviazione std dei residui) sia pressoché invariata nei tre modelli --> l'ampiezza degli intervalli è grossomodo invariata, 
## eventuali migliori percentuali di coverage da parte di qualche modello non saranno dovute ad un aumento dell'ampiezza dell'intervallo ma saranno legate al fatto cheil modello riuscirà effettivamente a spoegare meglio i dati! 

# da questa prima analisi sembra che il modello migliore per descrivere i dati potrebbe essere quello più numeroso

# il nostro interesse fino ad ora non è però tanto stato sulla bontà della previsione puntuale quanto invece
# sulla bontà degli intervalli di previsione  creati, vediamo dunque se ci sono differenze significative tra i modelli di diversa dimensione


fuori_int_100 = rep(NA, num_windows)

for(i in 1:num_windows) {
  
  fuori_int_100[i] = results_100[[i]]$percentuale_valori_fuori_int
  
}

mean_fuori_int_100= mean(fuori_int_100)



fuori_int_500 = rep(NA, num_windows)

for(i in 1:num_windows) {
  
  fuori_int_500[i] = results_500[[i]]$percentuale_valori_fuori_int
  
}

mean_fuori_int_500 = mean(fuori_int_500)
 


fuori_int_1000 = rep(NA, num_windows)

for(i in 1:num_windows) {
  
  fuori_int_1000[i] = results_1000[[i]]$percentuale_valori_fuori_int
  
}

mean_fuori_int_1000 = mean(fuori_int_1000)



fuori_int_3000 = rep(NA, num_windows)

for(i in 1:num_windows) {
  
  fuori_int_3000[i] = results_3000[[i]]$percentuale_valori_fuori_int
  
}

mean_fuori_int_3000 = mean(fuori_int_3000)


# grafichiamo i risultati

hist_fuori_int_100 = make_hist(fuori_int_100,  "pink",       "Errore fuori-I.P. (100)")
hist_fuori_int_500 = make_hist(fuori_int_500,  "lightgreen", "Errore fuori-I.P. (500)")
hist_fuori_int_1000 = make_hist(fuori_int_1000, "steelblue",  "Errore fuori-I.P. (1000)")
hist_fuori_int_3000 = make_hist(fuori_int_3000, "salmon",     "Errore fuori-I.P. (3000)")

x11()
grid.arrange(hist_fuori_int_100,
             hist_fuori_int_500, 
             hist_fuori_int_1000, 
             hist_fuori_int_3000, ncol = 2, nrow = 2)


## anche questa metrica conferma che un modello con più osservazioni performa meglio: il miglio AR(1) usa quindi tutti i dati del dataset per stimare i coeff
## N.B: stiamo ancora valutando la caapcità del modello di descrivere i dati che ha usato per stimare i propri coefficienti,
##      tra poco vedremo invece quale è migliore per fare previsione sul prossimo giorno 


##########################
## PRIMO MODELLO - test ##
##########################

# ora per la prima volta valutiamo la bontà del modello quando deve predirre osservazioni mai viste, fino ad ora quella che abbiamo valutato è stata solamente la capacità descrittiva del modello

# vogliamo vedere quanto peggiori la predizione andando avanti (non più solo il giorno immediatamente successivo ma anche dopodomani, dopodopodomani etc...)
# posso graficare come si alza l'errore e come dovrebbero cambiare le bande di previsione (allargarsi oltremodo)


# qual è la dimensione migliore del train per avere un modello che predice bene? coincide con quanto trovato prima? perché?
# va considerata anche l'ampiezza dell'intervallo! (intervalli ampissima faranno bene ma che senso hanno...)



############################################################
## MODELLO 1 bis - modifiche di progetto e considerazioni ##
############################################################

# valutiamo (e giustifichiamo) l'impatto di scelte di progetto quali:
# - dimensione del train set (quanto indietro mi serve guardare? comew cambiano le cose?) 
# - dimensione del test (quanto peggiorano le mie previsioni andando in là? ampiezza dell'intervallo e percentuale misurata di correttezza dello stesso)
# - AR(2), AR(3), ... , AR(n), sono predittori significativi anche i giorni che precedono l'ultimo? no!
# - e se, sorteggiato a caso un oggi, volessi fare previsioni su domani usando un AR(1), posso scrivere una funzione che mi aiuti a capire quanto mi conviene guardare indietro?

######################################
## confronto AR(1) con modello naif ##
######################################



################################################
## PRIMO MODELLO per il VALORE del PORTFOGLIO ##
################################################

## proviamo due diversi approcci su due diversi portafogli per evidenziare i limiti di un approccio semplice come il primo proposto (e come peggiorano le cose in certe situazioni)

# 1) applichiamo il modello migliore ottenuto fino ad ora (quello su cui ha lavorato nico forse) direttamente alla serie del valore complessivo del
#    portfoglio (ottenuta sommando (eventualmente con pesi) i valori di ciascun titolo)

# 2) applichiamo lo stesso modello singolarmente a ciascun serie e uniamo gli output per creare
#    un intervallo di previsione per il valore complessivo del portafoglio di domani


## creiamo questi modelli per due diversi portafogli: uno con titoli tra loro poco correlati e uno con titoli molto correlati (aziende tech americane ad es)


####################
### lavoro mamma ###
####################

rm(list=ls())

## lavoro mamma

vec = unlist(`goccia.(ita)`)

freq = rep(NA,length(vec))
count_met = 0



for(i in 1:length(vec)) 
  {
  
  if(vec[i] == "M") 
    { 
     count_met = count_met + 1
  }
  
  freq[i] = count_met / i  
}

x11()
plot(freq, type = "l")
abline(h = 0.59, col = 2, lty = 3)
abline(h = 0.60, col = 4, lty = 3)
abline(h = 0.58, col = 4, lty = 3)




x11()
# Limitiamo il grafico ai primi 300 dati
plot(freq[1:300], type = "l", col = "black", lwd = 1, 
     xlab = "Numero di osservazioni", ylab = "Frequenza Empirica", 
     main = "Convergenza della Frequenza Empirica")

# Aggiungiamo linee orizzontali per i valori di interesse
abline(h = freq[300], col = "red", lty = 3, lwd = 2)
abline(h = freq[300]+0.025, col = "green", lty = 3, lwd = 2)
abline(h = freq[300]-0.025, col = "green", lty = 3, lwd = 2)

# Aggiungiamo una legenda
legend("topright", legend = c("Frequenza Empirica", "Frequenza stimata", "Intervallo superiore", "Intervallo inferiore"),
       col = c("black", "red", "green", "green"), lty = c(1, 3, 3, 3), lwd = c(2, 2, 2, 2))




graphics.off()








