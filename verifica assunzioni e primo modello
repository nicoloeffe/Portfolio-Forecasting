
library(quantmod)
library(ggplot2)
library(e1071)
library(lmtest)
library(forecast)
library(tseries)
library(fitdistrplus)


rm(list=ls())


#import dei dati
start_date = "2010-01-01"
end_date = Sys.Date()


# Lista dei simboli da scaricare
titoli = c("SPY", "AGG", "VNQ", "GLD")  # Aggiungi i titoli che ti interessano


# Ciclo per scaricare i dati per ogni titolo
for(titolo in titoli) {
  getSymbols(titolo, src = "yahoo", from = start_date, to = end_date)
}




###########################
### analisi esplorativa ###
###########################


x11()
par(mfrow = c(2, 2))  

plot(AGG$AGG.Adjusted,
     main = "Andamento dell'AGG Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "steelblue")

plot(GLD$GLD.Adjusted,
     main = "Andamento del GLD Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "lightgreen")

plot(SPY$SPY.Adjusted,
     main = "Andamento del SPY Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "lightpink")

plot(VNQ$VNQ.Adjusted,
     main = "Andamento del VNQ Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "lightblue")


## non sembra assolutamente essere una serie stazionaria c'è un trend e in generale l'andamento sembra 'modellabile a tratti', potrebbe aver senso dividere i diversi periodi
## e creare diversi modelli per ciascun momento (periodi di fiducia, periodi di recessione, di fatto figli di diverse popolazioni (mistura)). anche la volatilità sembra essere variabile.



x11()
plot(VNQ$VNQ.Adjusted + SPY$SPY.Adjusted + GLD$GLD.Adjusted + AGG$AGG.Adjusted,
     main = "Andamento del valore del portfolio Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "purple")

## problema: così segue di fatto il titolo dal prezzo maggiore e si perde le altre variazioni --> creiamo un prtafoglio che pesi i titoli in modo tale da avere lo stesso valore di ogni titolo (in partenza almeno)

graphics.off()


## troviamo coefficienti che distribuiscano equamente il valore del portafoglio su ciascun titolo (assumiamo che il portafoglio valga 400)

coeff_AGG = 100/as.numeric(AGG$AGG.Adjusted[1])
coeff_SPY = 100/as.numeric(SPY$SPY.Adjusted[1])
coeff_GLD = 100/as.numeric(GLD$GLD.Adjusted[1])
coeff_VNQ = 100/as.numeric(VNQ$VNQ.Adjusted[1])

x11()
plot(coeff_VNQ*VNQ$VNQ.Adjusted + coeff_SPY*SPY$SPY.Adjusted + coeff_GLD*GLD$GLD.Adjusted + coeff_AGG*AGG$AGG.Adjusted,
     main = "Andamento del valore del portfolio Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "purple")

## in questo modo il portafoglio è inizialmente equamente diviso in valore tra ciascun titolo (segue poi l'andamento di SPY perché questo titolo cresce in valore più di tutti gli altri)


## plotto istogrammi dei valori assunti da ciascun titolo

x11()
par(mfrow = c(2, 2))  

hist(AGG$AGG.Adjusted, breaks = 100, col = "steelblue", 
     main = "Distribuzione dei valori giornalieri AGG ", xlab = "Valore titolo")

hist(SPY$SPY.Adjusted, breaks = 100, col = "lightpink", 
     main = "Distribuzione dei rendimenti giornalieri SPY", xlab = "Rendimento")

hist(GLD$GLD.Adjusted, breaks = 100, col = "lightgreen", 
     main = "Distribuzione dei rendimenti giornalieri GLD", xlab = "Rendimento")

hist(VNQ$VNQ.Adjusted, breaks = 100, col = "lightblue", 
     main = "Distribuzione dei rendimenti giornalieri VNQ", xlab = "Rendimento")

## le distribuzione sono chiaramente multimodali, questo potrebbe essere dovuto a diversi regimi di mercato, ad esempio:
# - fasi di crescita economica (periodi di maggiore ottimismo e crescita)
# - periodi di recessione (periodi di alta volatilità e stress nei mercati)
# - periodi di stabilità del valore del titolo
# La multimodalità indica che i dati potrebbero provenire da diverse popolazioni o regimi economici (come diverse fasi del mercato finanziario). 
# Potremmo considerare la possibilità di suddividere i dati in periodi di espansione e contrazione economica. 
# Si puo vedere anche dal grafico della serie storica: ci sono periodi di scarsa volatilità in cui il valore del titolo si assesta attorno a un numero preciso.



## stessa cosa per il valore aggregato del portafoglio:

x11()
hist(coeff_VNQ*VNQ$VNQ.Adjusted + coeff_SPY*SPY$SPY.Adjusted + coeff_GLD*GLD$GLD.Adjusted + coeff_AGG*AGG$AGG.Adjusted, breaks = 100, col = "purple", 
     main = "Distribuzione dei rendimenti giornalieri portfoglio", xlab = "Rendimento")



graphics.off()


####################################################
### verifica assunzioni per la serie storica AGG: ##
####################################################

## per comodità racchiudo nell'oggetto 'serie' i dati che considero (in questo modo il codice rimane uguale, senza bisogno di andare a 
## sostituire con il nome di un altro titolo in varie parti del codice, sarà sufficiente riassegnare la variabile 'serie')

serie = as.data.frame(AGG)

serie$Data = index(AGG)

# Creiamo una matrice con solo la data e il valore Adjusted
serie = serie[, c("Data", "AGG.Adjusted")]

head(serie)




## spesso conviene studiare gli incrementi e non direttamente il valore del titolo (vediamo sotto perché)
incrementi = rep(NA, nrow(serie)-1)

for(i in 2:nrow(serie)) {
  incrementi[i-1] = serie[i,2]-serie[i-1,2]
}

serie_incrementi = serie[-1,]

serie_incrementi$AGG.Adjusted = incrementi

head(serie_incrementi)


## facciamo un paio di plot:

x11()
par(mfrow = c(2, 2))  

plot(serie,
     type = "l",
     main = "Andamento dell'AGG Adjusted PRICE",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "steelblue")

plot(serie_incrementi,
     type = "l",
     main = "Andamento degli INCREMENTI dell'AGG Adjusted Price",
     xlab = "Data",
     ylab = "Prezzo Adjusted",
     col = "darkgrey")

hist(serie$AGG.Adjusted, breaks = 100, col = "steelblue", 
     main = "Distribuzione dei valori giornalieri AGG ", xlab = "Valore titolo")

hist(serie_incrementi$AGG.Adjusted, breaks = 100, col = "darkgrey", 
     main = "Distribuzione dei rendimenti giornalieri AGG", xlab = "Rendimento")

## già si vede ad occhio che gli incrementi rispettano meglio le ipotesi di stazionarietà debole (e di distribuzione normale)
## di fatto sembra che la varianza non sia veramente costante nel tempo ma in ogni caso per i rendimenti l'ipotesi di stazionarietà
## è molto meno forte che per i valori dei titoli.

graphics.off()


###########################
## STAZIONARIETA DEBOLE? ##
###########################

## valutiamo quantitativamente le considerazioni qualitative appena fatte:

adf.test(serie[,2])

# dati NON stazionari (interpretazione e motivazione)

# guardiamo come sono fatti trend e stagionalità (ad occhio la presenza di un trend sembra innegabile):

serie_ts = ts(serie$AGG.Adjusted, frequency = 365)
# converto i dati in una serie temporale (TS)

decomposizione = decompose(serie_ts)

x11()
plot(decomposizione)  # mi sembra fin troppo maracta un stagionalità così, PUZZA!!


# la serie ha sia un trend evidente che una stagionalità, il che potrebbe indicare che un modello che cattura questi due effetti (come ARIMA con stagionalità, SARIMA, o un modello che combina trend e stagionalità) potrebbe funzionare bene.

# un modo per de-trendizzare la serie è sicuramente quello di passare ai rendimenti


# se passo ai rendimenti ho stazionarietà debole:

adf.test(serie_incrementi[,2])

##############################################################
## OSSERVAZIONI DISTRIBUITE MARGINALMENTE COME UNA NORMALE?###
##############################################################

# ?? siamo sicuri serva questa assunzione? nelle slide sta scritto così ma per quanto fatto fino ad ora non mi sembra che la normalità sia necessaria
# forse NON è NECESSARIA questa parte (peccato)

## analisi VALORI TITOLO

shapiro.test(serie[,2])  # H0: i dati sono distribuiti normalmente 
                         # pvalue basso rifiuto l'ipotesi nulla --> i dati NON sono distribuiti normalmente

x11()
qqnorm(serie[,2])  # creo Q-Q plot (confronto i quantili empirici dei dati con quelli teorici di una normale)
qqline(serie[,2]) #  linea di riferimento: i punti dovrebbero seguirla se fossero distribuiti normalmente

# sembrano essere approssimativamente normali nella parte centrale, ma deviano dalla normalità nelle code. 
# questo potrebbe significare che i dati hanno un comportamento più estremo in alcune situazioni (ad esempio, in periodi di volatilità elevata nei mercati finanziari)     

x11()
hist(serie[,2], breaks = 50, probability = TRUE, col = "lightblue", main = "Histogram with Normal Curve")
lines(density(serie[,2]), col = "red")  # Aggiungi la curva della densità
curve(dnorm(x, mean = mean(serie[,2]), sd = sd(serie[,2])), col = "darkgreen", add = TRUE)  # Curva normale teorica

## la distribuzione è chiaramente  multimodale, questo potrebbe essere dovuto a diversi regimi di mercato, ad esempio:
# - fasi di crescita economica (periodi di maggiore ottimismo e crescita)
# - periodi di recessione (periodi di alta volatilità e stress nei mercati)
# - periodi di stabilità del valore del titolo
# La multimodalità indica che i dati potrebbero provenire da diverse popolazioni o regimi economici (come diverse fasi del mercato finanziario). 
# Potremmo considerare la possibilità di suddividere i dati in periodi di espansione e contrazione economica, e testare la normalità separatamente per ciascun periodo.
# Si puo vedere anche dal grafico della serie storica: ci sono periodi di scarsa volatilità in cui il valore del titolo si assesta attorno a un numero preciso.

skewness(serie[,2]) # = -0.0027 --> la distribuzione dei dati è abbastanza simmetrica attorno alla sua media 
kurtosis(serie[,2]) # = -0.6973 < 0 --> la distribuzione ha una coda più leggera rispetto alla distribuzione normale.
                           # le code dei dati non sono particolarmente pesanti, ma potrebbero comunque esserci andamenti 
                           # anomali o periodi di volatilità elevata che non sono ben rappresentati da una distribuzione normale.


# dopo queste analisi concludiamo che la normalità dei dati è probabilmente un'assunzione forte




## analisi RENDIMENTI

shapiro.test(serie_incrementi[,2])  # H0: i dati sono distribuiti normalmente 
# pvalue basso rifiuto l'ipotesi nulla --> i dati NON sono distribuiti normalmente

x11()
qqnorm(serie_incrementi[,2])  # creo Q-Q plot (confronto i quantili empirici dei dati con quelli teorici di una normale)
qqline(serie_incrementi[,2]) #  linea di riferimento: i punti dovrebbero seguirla se fossero distribuiti normalmente

# sembrano essere approssimativamente normali nella parte centrale, ma deviano dalla normalità nelle code. 
# questo potrebbe significare che i dati hanno un comportamento più estremo in alcune situazioni (ad esempio, in periodi di volatilità elevata nei mercati finanziari)     

x11()
hist(serie_incrementi[,2], breaks = 50, probability = TRUE, col = "lightblue", main = "Histogram with Normal Curve")
lines(density(serie_incrementi[,2]), col = "red")  # Aggiungi la curva della densità
curve(dnorm(x, mean = mean(serie_incrementi[,2]), sd = sd(serie_incrementi[,2])), col = "darkgreen", add = TRUE)  # Curva normale teorica

## dai test precedenti risulta che l'assunzione di normalità sia ancora un'assunzione forte ma sembrano "più normali" dei valori
## giornalieri del titolo, i rendimenti (pur non essendo normali) ci si avvicinano di più.

skewness(serie_incrementi[,2]) # = -1.386 --> asimmetri negativa (distribuzione spostata verso sinistra), questo è tipico dei mercati finanziari: spesso i prezzi titolo subiscono cali più significativi rispetto ai rialzi.
kurtosis(serie_incrementi[,2]) # = 27.06 < 0 --> code molto più pesanti rispetto a una distribuzione normale. Ciò suggerisce la presenza di eventi rari (come i forti picchi o cali nei prezzi), che sono tipici nei mercati finanziari.


## l'assunzione di normalità sembra essere forte anche in questo caso, la distribuzione dei rendimenti non può essere approssimata con una normale
## perché la distribuzione dei dati segue una distribuzione che consente eventi estremi più frequentemente di quanto prevederebbe una normale.
## nel nostro studio non siamo però interessati a prevedere gli eventi estremi ma solamente a fare previsioni su intervalli di tempi vicini: vogliamo fare mediamente delle buone previsioni, non importa sbagliare di grosso occasionalmente (statistici non trader)

## in conclusione è meglio utilizzare i dati relativi ai rendimenti per i modelli, non quelli dei valori del titolo

##################################################
## autocorrelazione e autocorellazione parziale ##
##################################################


## plottiamo autocorrelazione e autocorrelazione parziale dei valori giornalieri del titolo:

x11()
par(mfrow = c(1, 2)) 

acf(serie[,2], main = "Correlogramma VALORE AGG", lag.max = 100)
# ogni osservazione dipende fortemente dalle precedenti, i dati sembrano seguire una forte relazione temporale,
# un modello di serie temporale potrebbe effettivamente essere utile per modellare i dati

pacf(serie[,2], main = "Correlogramma parziale VALORE AGG", lag.max = 100)
# c'è una forte autocorrelazione tra il valore corrente e il valore precedente ma dopo il lag 1, le correlazioni diventano molto più piccole:
# la mancanza di significatività a lag più alti suggerisce che i dati non dipendono da lag più lontani.



## plottiamo ora autocorrelazione e autocorrelazione parziale dei rendimenti giornalieri:

x11()
par(mfrow = c(1, 2)) 

acf(serie_incrementi[,2], main = "Correlogramma INCREMENTI AGG", lag.max = 100)
# la correlazione non è significativa per lag maggiori di 0: usare l'incremento di oggi per spiegare quello di domani potrebbe NON essere una buona idea

pacf(serie_incrementi[,2], main = "Correlogramma parziale INCREMENTI AGG", lag.max = 100)



## penso che la forte correlazione temporale tra i valori giornalieri del titolo sia solo apparenza (poca sostanza). 
## mi spiego meglio: forse se i rendimenti non sono correlati significa che fare previsione per domani è difficile, 
## la correlazione tra il valore del titolo oggi e domani dice solo che se oggi vale 100 domani varrà ancora più o meno così 
## (quindi l'unica cosa che mi dice è che mediamente il rendimento giornaliero è piccolo, non sa però stimare il rendimento di domani guardando i rendimenti passati) 


# riassumendo quanto visto fino ad ora:
# - i dati relativi al valore giornaliero del titolo non rispettano l'assunzione di stazionarietà mentre quelli dei rendimenti (quasi) si 
# - i valori giornalieri sono temporalmente correlati tra loro (tutta apparenza), i rendimenti non presentano correlazioni temporali significative (purtroppo) (soldi facili per tutti altrimenti)
#                                                             (mi spiego meglio nelle righe 436 e seguenti)

#####################################################################
## DEFINIZIONE FINESTRE PER TRAIN E TEST DEL MODELLO - possibilità ##
#####################################################################

## diversi modi per dividere i dati in train e test set, mi interessa definire qua delle funzioni che possano essere
## usate per dividere il dataset in varie finestre (ciascuna con dati di train e dati di test)

# possibili idee per individuare le finestre (avendo scelto su quanti giorni allenare il modello e su quanti fare previsione):
# - finestre attigue (disgiunte) dalla lunghezza costante
# - finestre scorrevoli (da decidere di quanto traslare la finestra ogni volta) (non più disgiunte ma parzialmente sovrapposte)
# - N finestre prese in modo randomico sull'orizzonte temporale (alcune totalmente disgiunte, altre parzialmente sovrapposte, tutto casuale)



# funzione per creare finestre disgiunte:

non_overlapping_windows = function(dataset_completo, train_size, test_size) 
{
  windows = list()
  
  for (start in seq(1, nrow(dataset_completo) - (train_size + test_size), by = train_size + test_size)) 
  {
    end_train = start + train_size - 1
    start_test = end_train + 1
    end_test = start_test + test_size - 1
    
    windows[[length(windows) + 1]] = list(
      train = dataset_completo[start:end_train,2],
      test = dataset_completo[start_test:end_test,2]
    )
  }
  
  return(windows)
}



# funzione per creare finestre scorrevoli:

sliding_windows = function(dataset_completo, train_size, test_size, step_size) 
  {
    windows = list()
  
    for (start in seq(1, nrow(dataset_completo) - (train_size + test_size), by = step_size)) 
      {
        end_train = start + train_size - 1
        start_test = end_train + 1
        end_test = start_test + test_size - 1
        
        windows[[length(windows) + 1]] = list(
          train = dataset_completo[start:end_train,2],
          test = dataset_completo[start_test:end_test,2]
        )
      }
    
  return(windows)
}



# funzione per creare finestre campionate casualmente:

random_windows = function(dataset_completo, train_size, test_size, number_of_windows) 
{
  windows = list()
  
  for (i in 1:number_of_windows) 
  {
    start_train = sample(1:(nrow(dataset_completo) - (train_size + test_size)), 1)
    end_train = start_train + train_size - 1
    start_test = end_train + 1
    end_test = start_test + test_size - 1
    
    windows[[length(windows) + 1]] = list(
      train = dataset_completo[start_train:end_train,2],
      test = dataset_completo[start_test:end_test,2]
    )
  }
  
  return(windows)
}



##########################
## PRIMO MODELLO: AR(1) ##
##########################

## N.B: abbiamo visto prima che la serie dei valori giornalieri non è stazionaria, sarebbe una scelta più felice quella di usare i rendimenti (esattamente ciò che faremo poi)
## voglio però prima mostrare come la forte correlazione tra i valori giornalieri del titolo di fatto non sia così utile (dice solo che i rendimenti sono piccoli in media)

## qua di fatto giustifico l'insinuazione fatta a riga 343 (correlazione forte ma informazione solo apparentemente significativa)

# creiamo una regressione AR(1) e analizziamo il modello risultante:

valore_precedente = serie[1:nrow(serie)-1,2] # escludo l'ultimo valore (non è il valore precedente di nessun altro)

primo_modello = lm(serie[-1,2] ~ valore_precedente)
# escludo il primo valore (non ha valori precedenti che possiamo usare per stimarlo)

# visualizziamo i parametri stimati:
summary(primo_modello)
# Rquadro elevato, il modello sembrerebbe spiegare bene la variabilità dei dati
# coeff significativo e MOLTO vicino a 1 --> in linea con quanto ci saremmo aspettati (per questo modello poco utile, sostanzialmente per domani predice il valore di oggi)
# intercetta significativa ma bassa, sembra indicare una leggera tendenza positiva ( in assenza di altri fattori, il valore del titolo ha una piccola crescita costante nel tempo)

## il valore precedente è molto significativo, l'elevata correlazione tra valori vicini è quindi confermata dal modello.
## quando dico che questa è tutta apparenza intendo dire che di fatto l'evidenza raccolta da questo modello non è nulla di sorprendente: a meno di eventi straordinari il valore di domani assomiglierà a quello di oggi (incrementi giornalieri piccoli in media)


# confrontiamo valori fittati e valori reali
fitted_values = c((0), primo_modello$fitted.values)

x11()
plot(serie[1:10,2], col = 1)
lines(fitted_values[1:10], col = 2)
## il modello è estremamente scadente: predice per domani il valore di oggi 
## viene il dubbio che includere altre isservazioni oltre a quella del giorno immediatamente 
## precedente potrebbe migliorare le cose (no! infatti, la PACF risulti “piatta” dopo il primo picco 
## proprio perché una volta usato il valore immediatamente precedente per spiegare quello successivo
## tutti i valori che vengono ancora prima non sono più significativamente correlati con quello considerato)

## mostriamo come a prima vista un modello lineare possa non sembrare una pessima idea pur presentando i suoi limiti:

coordinate = cbind(valore_precedente, serie[-1, 2])

x11()
plot(coordinate)
abline(a = primo_modello$coefficients[1], b = primo_modello$coefficients[2], lwd = 3, col = 3)


graphics.off()


# L'analisi del modello sarà così strutturata:
# - diagnostica dei residui
# - costruzione intervalli di previsione
# - valutazione della bontà del modello sui test set

# mostreremo infine quanto di fatto tale modello non riesca a catturare alcuna informazione particolarmente utile
# confrontandolo con un modello estremamente naif (valore_domani = valore_oggi + rnorm(mu_incrementi, sd_incrementi)) 

#########################################
## PRIMO MODELLO - diagnostica residui ##
#########################################

summary(primo_modello)
# a occhio i residui non sembrano problematici ma facciamo una diagnostica più precisa:

# Calcoliamo i residui e grafichiamoli
residui = primo_modello$residuals

x11()
plot(residui, 
     main = "Residui", 
     ylab = "Residui")
abline(h = 0, col = 2, lwd = 2)
# sembrerebbero grosso modo simmetrici intorno allo 0
# i valori estremi di errore sono associati a un crollo improvviso del valore del titolo, non si poteva prevedere
# non propriamente stazionari (a occhio, poi approfondiamo), ce lo aspettavamo (la volatilità del mercato è variabile)


## RESIDUI NORMALI?

# Grafico Q-Q per verificare la normalità dei residui:
x11()
qqnorm(residui)
qqline(residui, col = "red")
# deviazioni dalle code normali suggeriscono la presenza di eventi estremi che il modello non può catturare completamente (giornate matte del mercato)

# non è un problema se il modello non è perfetto nella previsione di eventi estremi, l'importante è che offra un buon adattamento complessivo, 
# considerando che nel contesto finanziario gli eventi estremi sono intrinsecamente difficili da prevedere. L'importante è che il modello 
# riesca a catturare la tendenza generale e a gestire le fluttuazioni normali del mercato


shapiro.test(residui) # H0: residui normali, p-value basso --> rifiuto H0 (normalità dei residui ipotesi forte)

x11()
hist(residui, breaks = 50, probability = TRUE, col = "lightblue", main = "Histogram with Normal Curve")
lines(density(residui), col = "red")  # Aggiungi la curva della densità
curve(dnorm(x, mean = mean(residui), sd = sd(residui)), col = "darkgreen", add = TRUE)  # Curva normale teorica
legend("topright", 
       legend = c("Densità empirica dei residui", "Densità Normale Teorica"), 
       col = c("red", "darkgreen"), 
       lty = 1, 
       cex = 0.8)

## più stretta al centro ma code pesanti, la normale non sembra approssimare been la distribuzione dei residui
## lo vediamo anche da skewness e kurtosis:


skewness(residui) # = -1.3391 --> asimmetria negativa (distribuzione spostata verso sinistra), tipico dei mercati finanziari: spesso i prezzi subiscono cali più significativi rispetto ai rialzi
kurtosis(residui) # = 26.691 >> 0 --> la distribuzione ha code molto più pesanti rispetto alla distribuzione normale.

## confermo che assumere la normalità dei residui potrebbe essere un'ipotesi forte, ribadisco però quanto 
## scritto in precedenza: le giornate matte dei mercati (corrispondenti a residui particolarmente alti e responsabili dello spessore delle code) 
## non sono ciò che il nostro modello vuole spiegare. alla luce di questa considerazione potrebbe essere obiettato che quelle osservazione
## dovrebbero essere tolte dal dataset ma a questa obiezione risponderei così:
## "non stiamo costruendo questo modello perché pensiamo possa spiegare bene i dati, è un modello estremamente semplice, ci interessa usarlo solo per far emergere degli aspetti dell'analisi che troviamo interessanti. 
##  l'obiettivo a questo punto del lavoro è solo quello di fare osservazioni opportune e rilevanti, solo più avanti proveremo a usare modelli più sofisticati per spiegare meglio i dati".


# Calcoliamo e plottiamo l'autocorrelazione dei residui
x11()
acf(residui, main = "Autocorrelazione dei Residui") 
# residui scorrelati, bene

# Test di Breusch-Pagan per eteroscedasticità:
bptest(primo_modello)  # P-value bassissimo --> rifiuto H0, residui eteroschedastici
                       # è quello che dicevamo prima, la volatilità del mercato è variabile, questo
                       # si ripercuote sul modulo degli errori.


## IN CONCLUSIONE:
## residui indipendenti ma NON normali (e nemmeno identicamente distribuiti, eteroschedasticità)

## ??possiamo comunque usare il modello lineare??

#########################################################
## PRIMO MODELLO- costruzione intervalli di previsione ##
#########################################################


# PREVISIONE del giorno successivo (puntuale)

previsione_puntuale = primo_modello$fitted.values


## DUE vie per individuare degli intervalli di previsione per i valori successivi:

# 1) approccio frequentista: l'intervallo di previsione (simmetrico) viene centrato sulla previsione puntuale fatta dal modello 
#    e la sua ampiezza viene determinata a partire dalla varianza dell'errore (assunto normale) e dal quantile legato al livello di confidenza richiesto per la previsione 

# 2) approccio bayesiano: consideriamo le osservazioni future come v.a. e utilizziamo la distribuzione a posteriori per calcolare gli intervalli di previsione. 

# ???? chiedere a mastrantonio se abbia senso scomodare l'approccio bayesiano ????


## approccio frequentista: 

# Calcolo una stima della deviazione standard e della media dei residui:
sd_residui = sd(residui)
mean_residui = mean(residui) 

# Determinazione del valore critico per un intervallo di confidenza del 95% (siamo sotto l'assunzione (forte) valutata prima di normalità dei dati)
alpha = 0.05
quantile = qnorm(1 - alpha/2, mean_residui, sd_residui)  

# Calcoliamo l'intervallo di previsione al 95%
limite_superiore = previsione_puntuale + quantile*sd_residui
limite_inferiore = previsione_puntuale - quantile*sd_residui

# Visualizziamo il risultato
x11()
plot(serie[2:50,2], col = 1, pch = 16, cex = 0.5)
lines(limite_inferiore[1:50], col = 2, type = 'l')
lines(limite_superiore[1:50], col = 2, type = 'l')
legend("topleft",
       legend = c("Serie reale", 1 - alpha),
       col    = c(1, 2),
       pch    = c(16, NA),
       lty    = c(NA, 1),
       pt.cex = c(0.5, NA),
       bty    = "n")


graphics.off()


## ??? DOVREI/POTREI CONSIDERARE ANCHE L'INCERTEZZA NELLA STIMA DEI PARAMETRI? (GPT DICEVA DI AGGIUNGERE UN TERMINE LEGATO ALL'ERRORE STANDARD NELLA STIMA DEI PARAMETRI DEL MODELLO)


# vediamo se veramente il valore vero sta nell'intervallo di previsione il 95% delle volte:

ampiezza_int = 2*(quantile*sd_residui)

valori_fuori_intervallo = 0

for(i in 2:nrow(serie))
{
  if(serie[i,2] > previsione_puntuale[i] + ampiezza_int | serie[i,2] < previsione_puntuale[i] - ampiezza_int)
    valori_fuori_intervallo = valori_fuori_intervallo + 1
}

percentuale_valori_fuori_int = valori_fuori_intervallo/nrow(serie)


## confrontiamo la percentuale osservatadi valori fuori dall'int con la percentuale teorica (alpha):
cat(sprintf("Alpha teorico: %.2f, Percentuale osservata di valori fuori intervallo: %.2f",
            alpha, percentuale_valori_fuori_int))

## percentuale osservata >> percentuale teorica --> gli intervalli, seppur siano molto larghi, non lo sono abbastanza!
## tale discrepanza è dovuta al fatto che i residui non si distribuiscono davvero normalmente: le code della loro distribuzione
## sono molto più pesanti (ricordo i plot e il valore elevatissimo di kurtosis)


## ???? forse ha senso andare a rivedere cosa avevamo fatto nell'esercitazione dei dugonghi... avevamo creato delle bande di previsione per un modello lineare anche lì 


##########################
## PRIMO MODELLO - test ##
##########################

# come prima cosa siamo interessati a vedere se costruire il modello su 100 dati o su 1000 fa la differenza (test set preso sempre uguale a 1)


# vogliamo vedere quanto peggiori la predizine andando avanti (non più solo il giorno immediatamente successivo ma anche dopodomani, dopodopodomani etc...)
# posso graficare come si alza l'errore e come dovrebbero cambiare le bande di previsione (allargarsi oltremodo)


############################################################
## MODELLO 1 bis - modifiche di progetto e considerazioni ##
############################################################

# valutiamo (e giustifichiamo) l'impatto di scelte di progetto quali:
# - dimensione del train set (quanto indietro mi serve guardare? comew cambiano le cose?) 
# - dimensione del test (quanto peggiorano le mie previsioni andando in là? ampiezza dell'intervallo e percentuale misurata di correttezza dello stesso)
# - AR(2), AR(3), ... , AR(n), sono predittori significativi anche i giorni che precedono l'ultimo? no!

################################################
## PRIMO MODELLO per il VALORE del PORTFOGLIO ##
################################################

## proviamo due diversi approcci su due diversi portafogli per evidenziare i limiti di un approccio semplice come il primo proposto (e come peggiorano le cose in certe situazioni)

# 1) applichiamo il modello migliore ottenuto fino ad ora (quello su cui ha lavorato nico forse) direttamente alla serie del valore complessivo del
#    portfoglio (ottenuta sommando (eventualmente con pesi) i valori di ciascun titolo)

# 2) applichiamo lo stesso modello singolarmente a ciascun serie e uniamo gli output per creare
#    un intervallo di previsione per il valore complessivo del portafoglio di domani


## creiamo questi modelli per due diversi portafogli: uno con titoli tra loro poco correlati e uno con titoli molto correlati (aziende tech americane ad es)


####################
### lavoro mamma ###
####################

rm(list=ls())

## lavoro mamma

vec = unlist(`goccia.(ita)`)

freq = rep(NA,length(vec))
count_met = 0



for(i in 1:length(vec)) 
  {
  
  if(vec[i] == "M") 
    { 
     count_met = count_met + 1
  }
  
  freq[i] = count_met / i  
}

x11()
plot(freq, type = "l")
abline(h = 0.59, col = 2, lty = 3)
abline(h = 0.60, col = 4, lty = 3)
abline(h = 0.58, col = 4, lty = 3)




x11()
# Limitiamo il grafico ai primi 300 dati
plot(freq[1:300], type = "l", col = "black", lwd = 1, 
     xlab = "Numero di osservazioni", ylab = "Frequenza Empirica", 
     main = "Convergenza della Frequenza Empirica")

# Aggiungiamo linee orizzontali per i valori di interesse
abline(h = freq[300], col = "red", lty = 3, lwd = 2)
abline(h = freq[300]+0.025, col = "green", lty = 3, lwd = 2)
abline(h = freq[300]-0.025, col = "green", lty = 3, lwd = 2)

# Aggiungiamo una legenda
legend("topright", legend = c("Frequenza Empirica", "Frequenza stimata", "Intervallo superiore", "Intervallo inferiore"),
       col = c("black", "red", "green", "green"), lty = c(1, 3, 3, 3), lwd = c(2, 2, 2, 2))




graphics.off()








